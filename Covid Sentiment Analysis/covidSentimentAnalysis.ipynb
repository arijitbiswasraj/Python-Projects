{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Fri Jul 13 10:46:10 +0000 2012\", \"default_profile\": true, \"favourites_count\": 1, \"followers_count\": 1, \"friends_count\": 6, \"id\": 634480501, \"id_str\": \"634480501\", \"name\": \"White Sleep\", \"profile_background_color\": \"C0DEED\", \"profile_background_image_url\": \"http://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_background_image_url_https\": \"https://abs.twimg.com/images/themes/theme1/bg.png\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/2393717288/coolskull_normal.jpg\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/2393717288/coolskull_normal.jpg\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_border_color\": \"C0DEED\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"profile_use_background_image\": true, \"screen_name\": \"WhiteSleep13\", \"status\": {\"created_at\": \"Tue Feb 26 14:39:01 +0000 2019\", \"id\": 1100404876599672832, \"id_str\": \"1100404876599672832\", \"in_reply_to_screen_name\": \"yeashahrukh\", \"in_reply_to_status_id\": 1100402112024829952, \"in_reply_to_user_id\": 1099654338572619776, \"lang\": \"en\", \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"text\": \"@yeashahrukh Exactly with donated planes HAHAHA\"}, \"statuses_count\": 8}\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "\n",
    "# initialize api instance\n",
    "twitter_api = twitter.Api(consumer_key='dPgoCHhfjR5ffK1AlAKUABt72',\n",
    "                        consumer_secret='gtMC9sBjlrxU5CZiGrRpbicbGD57v5OtcbKHmdHTzy29hdsaJX',\n",
    "                        access_token_key='634480501-pSKdygTJs4iXCYmE17h6pUMVz7HeMgZbjtqzuQtd',\n",
    "                        access_token_secret='rWaYZ2hQiMtH2A8wlSz7kA9PxMPndf1a1V7gSPxgOKkeF')\n",
    "\n",
    "# test authentication\n",
    "print(twitter_api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTestSet(search_keyword):\n",
    "    try:\n",
    "        tweets_fetched = twitter_api.GetSearch(search_keyword, count = 100)\n",
    "        \n",
    "        print(\"Fetched \" + str(len(tweets_fetched)) + \" tweets for the term \" + search_keyword)\n",
    "        \n",
    "        return [[status.text, None] for status in tweets_fetched]\n",
    "    except:\n",
    "        print(\"Unfortunately, something went wrong..\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search keyword:corona virus\n",
      "Fetched 100 tweets for the term corona virus\n",
      "[['कांग्रेस विधायक की गाड़ी में मिली विदेशी शराब\\n(@rohit_manas ) #Bihar\\nhttps://t.co/jdEZmQkfAy', None], ['i’m just gonna say it, I hate corona virus, I hate quarantine', None], ['WHO menyatakan bahwa virus Corona (COVID-19) mungkin tidak akan pernah hilang dan penduduk Bumi harus belajar untuk… https://t.co/ozQ4vv0QUP', None], ['@GallagherCiaran Corona is like crunching a very fine, pale seashell. Virus is spring onion crisps.', None]]\n"
     ]
    }
   ],
   "source": [
    "search_term = input(\"Enter a search keyword:\")\n",
    "testDataSet = buildTestSet(search_term)\n",
    "\n",
    "print(testDataSet[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Biswas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Biswas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            label = 'negative'\n",
    "            if(tweet[1]!=None):\n",
    "                if float(tweet[1])>0:\n",
    "                    label = 'positive'\n",
    "            processedTweets.append((self._processTweet(tweet[0]),label))\n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self, tweet):\n",
    "        tweet = tweet.lower() # convert text to lower-case\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "        tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "        tweet = word_tokenize(tweet) # remove repeated characters (helloooooooo into hello)\n",
    "        return [word for word in tweet if word not in self._stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "tweet_data = []\n",
    "with open(\"ct1_tweets_label.csv\", 'r',encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    header = next(csv_reader)\n",
    "    print(\"header  = \")\n",
    "    print(header)\n",
    "    for row in csv_reader:\n",
    "        tweet_data.append([row[0],row[1].rstrip()])\n",
    "        print([row[0],row[1].rstrip()])\n",
    "print(len(tweet_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['rt', 'lot', 'would', 'actually', 'benefit', 'taking', 'good', 'look', 'introspectively', 'issues', 'women', 'gros…'], 'positive'), (['rt', 'day', '5', 'rediscovered', 'farming'], 'positive'), (['rt', 'mayorkun', 'somewhere', 'studio', 'singing', '``', 'ramona', 'body', 'dey', 'killing', 'person', 'like', 'corona', \"''\"], 'negative'), (['rt', 'appreciate', 'pak', 'jokowi', 'pak', 'prabowo', 'moves', 'fight', 'corona', 'never', 'die-hard', 'fan', 'hater', 'j…'], 'negative')]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "tweetProcessor = PreProcessTweets()\n",
    "preprocessedTrainingSet = tweetProcessor.processTweets(tweet_data[0:1000])\n",
    "preprocessedTestSet = tweetProcessor.processTweets(testDataSet)\n",
    "\n",
    "\n",
    "preprocessedTrainingSet = preprocessedTrainingSet[0:1000]\n",
    "print(preprocessedTrainingSet[0:4])\n",
    "print(len(preprocessedTrainingSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "def buildVocabulary(preprocessedTrainingData):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, sentiment) in preprocessedTrainingData:\n",
    "        all_words.extend(words)\n",
    "\n",
    "    wordlist = nltk.FreqDist(all_words)\n",
    "    word_features = wordlist.keys()\n",
    "    \n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words=set(tweet)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word]=(word in tweet_words)\n",
    "    return features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can extract the features and train the classifier \n",
    "word_features = buildVocabulary(preprocessedTrainingSet)\n",
    "trainingFeatures=nltk.classify.apply_features(extract_features,preprocessedTrainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBayesClassifier=nltk.NaiveBayesClassifier.train(trainingFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in preprocessedTestSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Negative Sentiment\n",
      "Negative Sentiment Percentage = 99.0%\n"
     ]
    }
   ],
   "source": [
    "# get the majority vote\n",
    "if NBResultLabels.count('positive') > NBResultLabels.count('negative'):\n",
    "    print(\"Overall Positive Sentiment\")\n",
    "    print(\"Positive Sentiment Percentage = \" + str(100*NBResultLabels.count('positive')/len(NBResultLabels)) + \"%\")\n",
    "else: \n",
    "    print(\"Overall Negative Sentiment\")\n",
    "    print(\"Negative Sentiment Percentage = \" + str(100*NBResultLabels.count('negative')/len(NBResultLabels)) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.05032427673372076, 1: 0.00011557718379945765}\n"
     ]
    }
   ],
   "source": [
    "# Example of calculating class probabilities\n",
    "from math import sqrt\n",
    "from math import pi\n",
    "from math import exp\n",
    "\n",
    "# Split the dataset by class values, returns a dictionary\n",
    "def separate_by_class(dataset):\n",
    "    separated = dict()\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        class_value = vector[-1]\n",
    "        if (class_value not in separated):\n",
    "            separated[class_value] = list()\n",
    "        separated[class_value].append(vector)\n",
    "    return separated\n",
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "\n",
    "# Calculate the standard deviation of a list of numbers\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "    return sqrt(variance)\n",
    "\n",
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "def summarize_dataset(dataset):\n",
    "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "    del(summaries[-1])\n",
    "    return summaries\n",
    "\n",
    "# Split dataset by class then calculate statistics for each row\n",
    "def summarize_by_class(dataset):\n",
    "    separated = separate_by_class(dataset)\n",
    "    summaries = dict()\n",
    "    for class_value, rows in separated.items():\n",
    "        summaries[class_value] = summarize_dataset(rows)\n",
    "    return summaries\n",
    "\n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "    probabilities = dict()\n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "        for i in range(len(class_summaries)):\n",
    "            mean, stdev, _ = class_summaries[i]\n",
    "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "    return probabilities\n",
    "def predict(summaries, row):\n",
    "    probabilities = calculate_class_probabilities(summaries, row)\n",
    "    best_label, best_prob = None, -1\n",
    "    for class_value, probability in probabilities.items():\n",
    "        if best_label is None or probability > best_prob:\n",
    "            best_prob = probability\n",
    "            best_label = class_value\n",
    "    return best_label\n",
    " \n",
    "# Naive Bayes Algorithm\n",
    "def naive_bayes(train, test):\n",
    "    summarize = summarize_by_class(train)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict(summarize, row)\n",
    "        predictions.append(output)\n",
    "    return(predictions)\n",
    " \n",
    "\n",
    "# Test calculating class probabilities\n",
    "dataset = [[3.393533211,2.331273381,0],\n",
    "           [3.110073483,1.781539638,0],\n",
    "           [1.343808831,3.368360954,0],\n",
    "           [3.582294042,4.67917911,0],\n",
    "           [2.280362439,2.866990263,0],\n",
    "           [7.423436942,4.696522875,1],\n",
    "           [5.745051997,3.533989803,1],\n",
    "           [9.172168622,2.511101045,1],\n",
    "           [7.792783481,3.424088941,1],\n",
    "           [7.939820817,0.791637231,1]]\n",
    "summaries = summarize_by_class(dataset)\n",
    "probabilities = calculate_class_probabilities(summaries, dataset[0])\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall tweet analysis negative with percentage:0.73\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ftable = pd.read_csv('ftable.csv')\n",
    "ftable = ftable[ftable['word'] != 'sad']\n",
    "ftable = ftable[ftable['word'] != 'sad,']\n",
    "ftable = ftable[ftable['word'] != ':(']\n",
    "ftable = ftable[ftable['word'] != 'fun']\n",
    "ftable = ftable[ftable['word'] != 'happy,']\n",
    "ftable = ftable[ftable['word'] != 'happy']\n",
    "ftable = ftable.drop_duplicates(subset = 'word')\n",
    "\n",
    "#test = 'corona is not leaving to know what to do anymore'\n",
    "#got this number from dataset creator\n",
    "positive_instance = 617\n",
    "negative_instance = 265.0\n",
    "pc =0 \n",
    "nc =0\n",
    "for row in preprocessedTestSet:\n",
    "    \n",
    "    test_words = row[0]\n",
    "\n",
    "    prob_positive = float(positive_instance/(positive_instance+negative_instance))\n",
    "    prob_negative = 1 - prob_positive\n",
    "\n",
    "    pos_word = 1.0*prob_positive\n",
    "    neg_word = 1.0*prob_negative\n",
    "    for i in range(len(test_words)):\n",
    "        word = test_words[i]\n",
    "        #print(word)\n",
    "        index_val = ftable.index[ftable['word'] == word]\n",
    "        if (len(index_val) > 0):\n",
    "            #print(index_val[0])\n",
    "            pos_val = ftable['positive'].iloc[index_val[0]]\n",
    "            neg_val = ftable['negative'].iloc[index_val[0]]\n",
    "            pos_word = pos_word * pos_val/positive_instance\n",
    "            neg_word = neg_word * neg_val/negative_instance\n",
    "\n",
    "    if pos_word > neg_word:\n",
    "        pc+=1\n",
    "\n",
    "    else:\n",
    "        nc+=1\n",
    "if(nc > pc):\n",
    "    print(\"Overall tweet analysis negative with percentage:\" +str(nc/100))\n",
    "else:\n",
    "    print(\"Overall tweet analysis positive with percentage:\" +str(pc/100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
